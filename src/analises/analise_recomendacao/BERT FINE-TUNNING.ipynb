{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "from bd import nova_conexao\n",
    "from psycopg2.errors import ProgrammingError\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "from utils import ndcg\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtendo os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Obtendo os dados e colocando-as em um dataframe\n",
    "def obter_dados_bd():\n",
    "    sql_select_dados = \"select eua.id, eua.curriculo_texto, eva.id, eva.texto, na.nota \\\n",
    "    from nota_analise na inner join emprega_usuario_analise eua on na.usuario_id = eua.id \\\n",
    "    inner join emprega_vaga_analise eva on na.vaga_id = eva.id \\\n",
    "    order by na.vaga_id\"\n",
    "    curriculos = []\n",
    "    vagas = []\n",
    "    notas = []\n",
    "    with nova_conexao() as conexao:\n",
    "        try:\n",
    "            cursor = conexao.cursor()\n",
    "            cursor.execute(sql_select_dados)\n",
    "            resultado = cursor.fetchall()\n",
    "        except ProgrammingError as e:\n",
    "            print(f'Erro: {e.msg}')\n",
    "        else:\n",
    "            for row in resultado:\n",
    "                curriculos.append(row[1])\n",
    "                vagas.append(row[3])\n",
    "                notas.append(row[4])\n",
    "            conexao.commit()\n",
    "\n",
    "    df_dados = pd.DataFrame(\n",
    "        {\n",
    "            'curriculos' : curriculos,\n",
    "            'vagas' : vagas,\n",
    "            'notas' : notas\n",
    "        })\n",
    "    df_dados.to_csv('dados.csv', index=False, encoding='utf-8')\n",
    "    return df_dados\n",
    "\n",
    "\n",
    "# Lendo e salvando para CSV\n",
    "def obter_dados_csv():\n",
    "    df_dados = pd.read_csv('dados.csv')\n",
    "    df_dados.to_csv('dados.csv', index=False, encoding='utf-8')\n",
    "    return df_dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dados = obter_dados_csv()\n",
    "df_dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removendo os dígitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dados[\"curriculos\"] = df_dados[\"curriculos\"].apply(lambda x: re.sub('\\d+', '', x))\n",
    "df_dados[\"vagas\"] = df_dados[\"vagas\"].apply(lambda x: re.sub('\\d+', '', x))\n",
    "df_dados[\"notas\"] = df_dados[\"notas\"].apply(lambda x: x-1) # Ajustando o target porque o torch utiliza as classes começando do 0 [0, num_labels-1]\n",
    "df_dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurando o Transformer e o Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma em um dataset do HuggingFace para usar a sua função map\n",
    "dados = Dataset.from_pandas(df_dados)\n",
    "dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(inputs):\n",
    "    return tokenizer(\n",
    "        inputs['curriculos'], inputs['vagas'], truncation=True, return_tensors='pt', padding='max_length', max_length=128\n",
    "        )\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "dados_tokenizados = dados.map(tokenize_function, batched=True)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('neuralmind/bert-base-portuguese-cased', num_labels=5, hidden_dropout_prob = 0.1, output_hidden_states=True)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_tokenizados = dados_tokenizados.remove_columns(['curriculos', 'vagas'])\n",
    "dados_tokenizados = dados_tokenizados.rename_column('notas', 'labels')\n",
    "dados_tokenizados.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_tokenizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparando o treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(eval_dataloader, desc='validação'):\n",
    "\n",
    "    metric_acc = evaluate.load(\"accuracy\")\n",
    "    metric_f1_micro = evaluate.load(\"f1\")\n",
    "    metric_f1_macro = evaluate.load(\"f1\")\n",
    "    metric_f1_weighted = evaluate.load(\"f1\")\n",
    "    metric_roc_auc = evaluate.load(\"roc_auc\", \"multiclass\")\n",
    "    progress_bar2 = tqdm(range(len(eval_dataloader)), desc=f'Validação no conjunto de {desc}')\n",
    "    model.eval()\n",
    "\n",
    "    for batch in eval_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "        softmax_roc_auc = torch.softmax(outputs.logits, dim=-1) # Utilizado para o ROC_AUC que não usa os logits, mas a saída do softmax (a prob de cada classe, de modo que a soma de todas as prob dê 1)\n",
    "        metric_acc.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        metric_f1_micro.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        metric_f1_macro.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        metric_f1_weighted.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        metric_roc_auc.add_batch(prediction_scores=softmax_roc_auc, references=batch[\"labels\"])\n",
    "        \n",
    "        progress_bar2.update(1)\n",
    "        \n",
    "    acc = metric_acc.compute()\n",
    "    f1_micro = metric_f1_micro.compute(average=\"micro\")\n",
    "    f1_macro = metric_f1_macro.compute(average=\"macro\")\n",
    "    f1_weighted = metric_f1_weighted.compute(average=\"weighted\")\n",
    "    roc_auc =  metric_roc_auc.compute(multi_class='ovo') #'ovo' é melhor para datasets desbalanceados\n",
    "    \n",
    "    return acc, f1_micro, f1_macro, f1_weighted, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dados_tokenizados=dados_tokenizados.train_test_split(0.2, seed=42)\n",
    "test_dataloader = DataLoader(dados_tokenizados['test'], batch_size=8)\n",
    "\n",
    "# A partir do treino, obtém os dados de validação\n",
    "dados_tokenizados=dados_tokenizados['train'].train_test_split(0.2, seed=42)\n",
    "train_dataloader = DataLoader(dados_tokenizados['train'], shuffle=True, batch_size=8)\n",
    "val_dataloader = DataLoader(dados_tokenizados['test'], batch_size=8)\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "dados_tokenizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress_bar = tqdm(range(num_training_steps), desc='Treino')\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "    train_acc, train_f1_micro, train_f1_macro, train_f1_weighted, train_roc_auc = evaluate_model(train_dataloader)\n",
    "    val_acc, val_f1_micro, val_f1_macro, val_f1_weighted, val_roc_auc = evaluate_model(val_dataloader)\n",
    "    print(f'Resultados com o conjunto de treino na época {epoch}')\n",
    "    print(f'Acurácia: {train_acc}')\n",
    "    print(f'Micro-F1: {train_f1_micro}')\n",
    "    print(f'Macro-F1: {train_f1_macro}')\n",
    "    print(f'Weighted-F1: {train_f1_weighted}')\n",
    "    print(f'ROC-AUC: {train_roc_auc}')\n",
    "\n",
    "    print(f'Resultados com o conjunto de validação na época {epoch}')\n",
    "    print(f'Acurácia: {val_acc}')\n",
    "    print(f'Micro-F1: {val_f1_micro}')\n",
    "    print(f'Macro-F1: {val_f1_macro}')\n",
    "    print(f'Weighted-F1: {val_f1_weighted}')\n",
    "    print(f'ROC-AUC: {val_roc_auc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # a melhor métrica será macro f1 porque o dataset está desbalanceado\n",
    "    # para a métrica f1_weighted, devemos selecionar melhor os dados de validação para colocar mais amostras onde se tem nota 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
